{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re  \n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from pymystem3 import Mystem\n",
    "from scipy.spatial import distance \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the folder name\n",
      "Working folder: C:/Users/v.kozlovskiy/Desktop/DATAS/from_aug24/_ekklesiat_wc_and_other/to_git/\n"
     ]
    }
   ],
   "source": [
    "stopwhile = 0\n",
    "print()\n",
    "print('Enter the folder name')\n",
    "\n",
    "while stopwhile != 1:\n",
    "    path = input()\n",
    "\n",
    "    try:\n",
    "        path = path.replace('\\\\', '/') + '/'\n",
    "        print('Working folder:', path)\n",
    "        text_df = pd.read_excel(path + 'text_for_analysis_rus.xlsx')\n",
    "        text_df = pd.read_excel(path + 'text_for_analysis_eng.xlsx')\n",
    "        \n",
    "        stopwhile = 1\n",
    "    except: \n",
    "        print('Error in entering the working folder name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_language(text):\n",
    "    rus = re.sub(r'[^а-я\\']', ' ', text)\n",
    "    rus = rus.split()\n",
    "    rus = ''.join(rus)\n",
    "    eng = re.sub(r'[^a-z\\']', ' ', text)\n",
    "    eng = eng.split()\n",
    "    eng = ''.join(eng)\n",
    "\n",
    "#    print(len(rus), rus)\n",
    "#    print(len(eng), eng)\n",
    "\n",
    "    if len(rus) == 0:\n",
    "        print('Your input is english')\n",
    "        lang = 1\n",
    "    else: \n",
    "        if len(eng) == 0:\n",
    "            print('Your input is russian')\n",
    "            lang = 0\n",
    "        else:\n",
    "            if len(rus) > len(eng):\n",
    "                print('Your input is generally russian text')\n",
    "                lang = 0\n",
    "            else: \n",
    "                print('Your input is generally english text')\n",
    "                lang = 1\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your phrase:\n",
      "Inputed text: wisdom истина SUN 12 lABor\n",
      "Your input is generally english text\n"
     ]
    }
   ],
   "source": [
    "print('Enter your phrase:')\n",
    "input_text = input()\n",
    "print('Inputed text:', input_text)\n",
    "lang = check_language(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open prepared file\n",
    "if lang == 0:\n",
    "    text_df = pd.read_excel(path + 'text_for_analysis_rus.xlsx')\n",
    "else:\n",
    "    text_df = pd.read_excel(path + 'text_for_analysis_eng.xlsx')    \n",
    "    \n",
    "original = list(text_df['original'])\n",
    "text = list(text_df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_by_text(text, lang):\n",
    "    # preprocessing like before making wordcloud\n",
    "    if lang == 0:\n",
    "        russian_stopwords = stopwords.words(\"russian\")\n",
    "        russian_stopwords.extend(['это', 'все', 'вс'])\n",
    "        union = ' '.join(text)\n",
    "        text_tokens = word_tokenize(union)\n",
    "        text_tokens = [token.strip() for token in text_tokens if token not in russian_stopwords]\n",
    "    else:\n",
    "        english_stopwords = stopwords.words(\"english\")\n",
    "        union = ' '.join(text)\n",
    "        text_tokens = word_tokenize(union)\n",
    "        text_tokens = [token.strip() for token in text_tokens if token not in english_stopwords]\n",
    "    \n",
    "    # select unique tokens\n",
    "    uniq_tokens = pd.Series(text_tokens).unique()\n",
    "\n",
    "    # matrix for analysis in first dimension strings in text in second unique tokens\n",
    "    matrix = np.zeros((len(text), len(uniq_tokens)))\n",
    "    print('Number of strings in text:', len(text))\n",
    "    print('Number of tokens in text:', len(text_tokens))\n",
    "    print('Number of unique tokens in text:', len(uniq_tokens))\n",
    "    print('Matrix shape:', matrix.shape)\n",
    "    \n",
    "    # cycle for fill the matrix (if corresponding token is in sentence 1, if no 0)\n",
    "    for sentence in range(len(text)):\n",
    "        if lang == 0:\n",
    "            single_sentence = re.split('[^а-я]', text[sentence])\n",
    "        else:\n",
    "            single_sentence = re.split('[^a-z]', text[sentence])   \n",
    "\n",
    "        #print(single_sentence)\n",
    "        for word in single_sentence:\n",
    "            for i in range(len(uniq_tokens)):\n",
    "                if word == uniq_tokens[i]:\n",
    "                    #print(word, i)\n",
    "                    matrix[sentence, i] = 1\n",
    "\n",
    "    return matrix, uniq_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text filtration and lemmatization\n",
    "def filtration(text, lang):\n",
    "  text = text.lower()\n",
    "  if lang == 0:\n",
    "      text = ''.join(m.lemmatize(text))\n",
    "      text = re.sub(r'[^а-я\\']', ' ', text)\n",
    "  else:\n",
    "      text = ''.join(lemmatizer.lemmatize(text))\n",
    "      text = re.sub(r'[^a-z\\']', ' ', text)\n",
    "  text = text.split()\n",
    "  print('ok')\n",
    "  return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding of inputed text with unique tokens to compare it with matrix\n",
    "def sentence_encoder(text, uniq_tokens, lang):\n",
    "    text = filtration(text, lang)\n",
    "    #print(text)\n",
    "    if len(text) > len(uniq_tokens):\n",
    "        text = text[:len(uniq_tokens)]\n",
    "    encoded = np.zeros(len(uniq_tokens))\n",
    "    for word in text.split():\n",
    "        for i in range(len(uniq_tokens)):\n",
    "            if word == uniq_tokens[i]:\n",
    "                print(word, i)\n",
    "                encoded[i] = 1\n",
    "    return encoded, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for counting cousine distance and define closest sentences from text and index or closest sentence\n",
    "def nearest_phrase(encoded_input, matrix):\n",
    "\n",
    "    index_list = []  # list for indexes\n",
    "    distance_list = []  # list for cousine distances\n",
    "    for i in range(matrix.shape[0]):  # cycle for compare encoded inputed phrase with phrases from file\n",
    "         index_list.append(i)   \n",
    "         distance_list.append(scipy.spatial.distance.cosine(encoded_input, matrix[i,:]))    # counting of cousine distance\n",
    "\n",
    "    # table with results\n",
    "    distance_table = pd.DataFrame([index_list, distance_list], index=['index', 'distance'])\n",
    "    distance_table = distance_table.T\n",
    "    #print(np.argmin(distance_table['distance']))\n",
    "    needable_index = np.argmin(distance_table['distance'])\n",
    "    distance_table = distance_table.sort_values(by='distance', ascending=True)\n",
    "    #print(distance_table.head(5))\n",
    "    \n",
    "    return distance_table, needable_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings in text: 222\n",
      "Number of tokens in text: 2523\n",
      "Number of unique tokens in text: 793\n",
      "Matrix shape: (222, 793)\n"
     ]
    }
   ],
   "source": [
    "matrix_a, uniq_tokens = matrix_by_text(text, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "wisdom 78\n",
      "sun 14\n",
      "Processed inputed text: wisdom sun labor\n"
     ]
    }
   ],
   "source": [
    "enc_input, phrase = sentence_encoder(input_text, uniq_tokens, lang)\n",
    "print('Processed inputed text:', phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 closest phrase from text:\n",
      "\n",
      "7:11 wisdom is good with an inheritance: and by it there is profit to them that see the sun. 0.42\n",
      "9:13 this wisdom have i seen also under the sun, and it seemed great unto me. 0.47\n",
      "4:7 then i returned, and i saw vanity under the sun. 0.65\n",
      "7:23 all this have i proved by wisdom: i said, i will be wise; but it was far from me. 0.68\n",
      "1:18 for in much wisdom is much grief: and he that increaseth knowledge increaseth sorrow. 0.71\n"
     ]
    }
   ],
   "source": [
    "distance_table, needable_index = nearest_phrase(enc_input, matrix_a)\n",
    "if distance_table['distance'][needable_index] > 0:\n",
    "    print()\n",
    "    print('5 closest phrase from text:')\n",
    "    print()\n",
    "    for i in distance_table['index'][:5]:\n",
    "        print(text_df['original'][i], round(distance_table['distance'][i],2))\n",
    "else:\n",
    "    print()\n",
    "    print('There are no close phrases in text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
