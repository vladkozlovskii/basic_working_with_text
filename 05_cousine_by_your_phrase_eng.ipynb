{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\v.kozlovskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re  \n",
    "import scipy\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "#from pymystem3 import Mystem\n",
    "#from wordcloud import WordCloud  \n",
    "\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/v.kozlovskiy/Desktop/DATAS/from_aug24/_ekklesiat_wc_and_other/to_git/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # lemmatizer for english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open prepared file\n",
    "text_df = pd.read_excel(path + 'text_for_analysis_eng.xlsx')\n",
    "original = list(text_df['original'])\n",
    "text = list(text_df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_by_text(text):\n",
    "    # preprocessing like before making wordcloud\n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "    russian_stopwords.extend(['это', 'все', 'вс'])\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    union = ' '.join(text)\n",
    "    text_tokens = word_tokenize(union)\n",
    "    text_tokens = [token.strip() for token in text_tokens if token not in english_stopwords]\n",
    "    \n",
    "    # select unique tokens\n",
    "    uniq_tokens = pd.Series(text_tokens).unique()\n",
    "\n",
    "    # matrix for analysis in first dimension strings in text in second unique tokens\n",
    "    matrix = np.zeros((len(text), len(uniq_tokens)))\n",
    "    print('Number of strings in text:', len(text))\n",
    "    print('Number of tokens in text:', len(text_tokens))\n",
    "    print('Number of unique tokens in text:', len(uniq_tokens))\n",
    "    print('Matrix shape:', matrix.shape)\n",
    "    \n",
    "    # cycle for fill the matrix (if corresponding token is in sentence 1, if no 0)\n",
    "    for sentence in range(len(text)):\n",
    "        single_sentence = re.split('[^a-z]', text[sentence])\n",
    "        #print(single_sentence)\n",
    "        for word in single_sentence:\n",
    "            for i in range(len(uniq_tokens)):\n",
    "                if word == uniq_tokens[i]:\n",
    "                    #print(word, i)\n",
    "                    matrix[sentence, i] = 1\n",
    "\n",
    "    return matrix, uniq_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text filtration and lemmatization of inputed text\n",
    "def filtration(text):\n",
    "  text = text.lower()\n",
    "  text = ''.join(lemmatizer.lemmatize(text))\n",
    "  text = re.sub(r'[^a-z\\']', ' ', text)\n",
    "  text = text.split()\n",
    "  print('ok')\n",
    "  return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding of inputed text with unique tokens to compare it with matrix\n",
    "def sentence_encoder(text, uniq_tokens):\n",
    "    text = filtration(text)\n",
    "    print(text)\n",
    "    if len(text) > len(uniq_tokens):\n",
    "        text = text[:len(uniq_tokens)]\n",
    "    encoded = np.zeros(len(uniq_tokens))\n",
    "    for word in text.split():\n",
    "        for i in range(len(uniq_tokens)):\n",
    "            if word == uniq_tokens[i]:\n",
    "                print(word, i)\n",
    "                encoded[i] = 1\n",
    "    return encoded, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for counting cousine distance and define closest sentences from text and index or closest sentence\n",
    "def nearest_phrase(encoded_input, matrix):\n",
    "\n",
    "    index_list = []  # list for indexes\n",
    "    distance_list = []  # list for cousine distances\n",
    "    for i in range(matrix.shape[0]):  # cycle for compare encoded inputed phrase with phrases from file\n",
    "         index_list.append(i)   \n",
    "         distance_list.append(scipy.spatial.distance.cosine(encoded_input, matrix[i,:]))    # counting of cousine distance\n",
    "\n",
    "    # table with results\n",
    "    distance_table = pd.DataFrame([index_list, distance_list], index=['index', 'distance'])\n",
    "    distance_table = distance_table.T\n",
    "    #print(np.argmin(distance_table['distance']))\n",
    "    needable_index = np.argmin(distance_table['distance'])\n",
    "    distance_table = distance_table.sort_values(by='distance', ascending=True)\n",
    "    #print(distance_table.head(5))\n",
    "    \n",
    "    return distance_table, needable_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings in text: 222\n",
      "Number of tokens in text: 2523\n",
      "Number of unique tokens in text: 793\n",
      "Matrix shape: (222, 793)\n"
     ]
    }
   ],
   "source": [
    "matrix_a, uniq_tokens = matrix_by_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputed text: wisdom sun trut\n"
     ]
    }
   ],
   "source": [
    "input_text = input()\n",
    "print('Inputed text:', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "wisdom sun trut\n",
      "wisdom 78\n",
      "sun 14\n",
      "Processed inputed text: wisdom sun trut\n"
     ]
    }
   ],
   "source": [
    "enc_input, phrase = sentence_encoder(input_text, uniq_tokens)\n",
    "print('Processed inputed text:', phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 closest phrase from text:\n",
      "\n",
      "7:11 wisdom is good with an inheritance: and by it there is profit to them that see the sun. 0.42\n",
      "9:13 this wisdom have i seen also under the sun, and it seemed great unto me. 0.47\n",
      "4:7 then i returned, and i saw vanity under the sun. 0.65\n",
      "7:23 all this have i proved by wisdom: i said, i will be wise; but it was far from me. 0.68\n",
      "1:18 for in much wisdom is much grief: and he that increaseth knowledge increaseth sorrow. 0.71\n"
     ]
    }
   ],
   "source": [
    "distance_table, needable_index = nearest_phrase(enc_input, matrix_a)\n",
    "print()\n",
    "print('5 closest phrase from text:')\n",
    "print()\n",
    "for i in distance_table['index'][:5]:\n",
    "    print(text_df['original'][i], round(distance_table['distance'][i],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
